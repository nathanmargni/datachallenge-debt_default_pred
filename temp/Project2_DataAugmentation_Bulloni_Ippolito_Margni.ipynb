{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30e5217",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740630eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries needed\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from shap import LinearExplainer, summary_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7addc492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the two datasets from the pickle files\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "X_train = pd.read_pickle(dir_path + \"\\\\data\\\\X_train.pkl\")\n",
    "X_test = pd.read_pickle(dir_path + \"\\\\data\\\\X_test.pkl\") \n",
    "\n",
    "X_train_norm = pd.read_pickle(dir_path + \"\\\\data\\\\X_train_norm.pkl\")\n",
    "X_test_norm = pd.read_pickle(dir_path + \"\\\\data\\\\X_test_norm.pkl\")\n",
    "\n",
    "X_train_log = pd.read_pickle(dir_path + \"\\\\data\\\\X_train_log.pkl\")\n",
    "X_test_log = pd.read_pickle(dir_path + \"\\\\data\\\\X_test_log.pkl\")\n",
    "\n",
    "y_train = pd.read_pickle(dir_path + \"\\\\data\\\\y_train.pkl\")\n",
    "y_test = pd.read_pickle(dir_path + \"\\\\data\\\\y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ac18df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns names\n",
    "column_names = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54599f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train norm:\n",
      "        LIMIT_BAL  SEX_MALE  AGE  PAY_9  PAY_8  PAY_7  PAY_6  PAY_5  PAY_4  \\\n",
      "21177   0.292929         1   31      0      0      0      0      0      0   \n",
      "23942   0.010101         1   24      0      0      0      0      0      0   \n",
      "\n",
      "       BILL_AMT_9  ...  EDUCATION_1  EDUCATION_2  EDUCATION_3  EDUCATION_4  \\\n",
      "21177    0.218131  ...            0            0            1            0   \n",
      "23942    0.160438  ...            0            1            0            0   \n",
      "\n",
      "       EDUCATION_5  EDUCATION_6  MARRIAGE_0  MARRIAGE_1  MARRIAGE_2  \\\n",
      "21177            0            0           0           0           1   \n",
      "23942            0            0           0           0           1   \n",
      "\n",
      "       MARRIAGE_3  \n",
      "21177           0  \n",
      "23942           0  \n",
      "\n",
      "[2 rows x 32 columns]\n",
      "X train log:\n",
      "        LIMIT_BAL  SEX_MALE  AGE  PAY_9  PAY_8  PAY_7  PAY_6  PAY_5  PAY_4  \\\n",
      "21177   0.470005         1   31      0      0      0      0      0      0   \n",
      "23942   0.039223         1   24      0      0      0      0      0      0   \n",
      "\n",
      "       BILL_AMT_9  ...  EDUCATION_1  EDUCATION_2  EDUCATION_3  EDUCATION_4  \\\n",
      "21177    0.150020  ...            0            0            1            0   \n",
      "23942    0.030977  ...            0            1            0            0   \n",
      "\n",
      "       EDUCATION_5  EDUCATION_6  MARRIAGE_0  MARRIAGE_1  MARRIAGE_2  \\\n",
      "21177            0            0           0           0           1   \n",
      "23942            0            0           0           0           1   \n",
      "\n",
      "       MARRIAGE_3  \n",
      "21177           0  \n",
      "23942           0  \n",
      "\n",
      "[2 rows x 32 columns]\n",
      "y train:\n",
      " 21177    0\n",
      "23942    0\n",
      "Name: default payment next month, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the data was imported correctly\n",
    "print(\"X train norm:\\n\", X_train_norm.head(2))\n",
    "print(\"X train log:\\n\", X_train_log.head(2))\n",
    "print(\"y train:\\n\", y_train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "802ff805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training into train-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570210de",
   "metadata": {},
   "source": [
    "### Test the performances of a Logistic Regression model with normalized and natural log transformed data, to choose one of the two to be used with the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28f2c5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with normalized transformed data:  0.8104\n",
      "Accuracy with natural log transformed data:  0.8112\n",
      "sklearn f1_score on normalized data:  0.3518687329079307\n",
      "sklearn f1_score on natural log transformed data:  0.3557779799818016\n"
     ]
    }
   ],
   "source": [
    "# Start testing models, starting with a simple logistic regression\n",
    "log_reg_norm = LogisticRegression(random_state=42, max_iter=10000)\n",
    "log_reg_log = LogisticRegression(random_state=42, max_iter=10000)\n",
    "\n",
    "log_reg_norm.fit(X_train_norm, y_train)\n",
    "log_reg_log.fit(X_train_log, y_train)\n",
    "\n",
    "print(\"Accuracy with normalized transformed data: \", log_reg_norm.score(X_test_norm, y_test))\n",
    "print(\"Accuracy with natural log transformed data: \", log_reg_log.score(X_test_log, y_test))\n",
    "\n",
    "y_pred_norm = log_reg_norm.predict(X_test_norm)\n",
    "y_pred_log = log_reg_log.predict(X_test_log)\n",
    "print(\"sklearn f1_score on normalized data: \", f1_score(y_test, y_pred_norm))\n",
    "print(\"sklearn f1_score on natural log transformed data: \", f1_score(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47bfe8b",
   "metadata": {},
   "source": [
    "From the F1-score and the accuracy it is clear the dataset is very unbalanced. \n",
    "\n",
    "We'll use SMOTENC to add samples from the least numerous classes and try again with a naive logistic regression.\n",
    "We'll use SMOTENC and not SMOTE as we have some categorical features encoded as numerical bianry variables, and we don't want to denaturalize them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6950bc",
   "metadata": {},
   "source": [
    "Try to oversample the dataset to reach a balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50c05440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples for each class: \n",
      "0    17491\n",
      "1     5009\n",
      "Name: default payment next month, dtype: int64\n",
      "Percentage of class 1 on the total: 22%\n",
      "Number of samples for each class: \n",
      "0    17491\n",
      "1    17491\n",
      "Name: default payment next month, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTENC to try to balance the dataset\n",
    "# We also tried to use RandomOverSampler, but SMOTENC seem to give better results, and we prefer it also because it generates new samples and doesn't just resample.\n",
    "value_counts = y_train.value_counts()\n",
    "tot_values = len(y_train)\n",
    "cat_columns = [1, 3, 4, 5, 6, 7, 8, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
    "\n",
    "print(\"Number of samples for each class: \\n\" + str(value_counts))\n",
    "print(\"Percentage of class 1 on the total: \" + str(round(value_counts[1]/tot_values*100)) + \"%\")\n",
    "\n",
    "smote = SMOTENC(cat_columns, k_neighbors=5, sampling_strategy=\"auto\")\n",
    "#smote= RandomOverSampler(sampling_strategy='auto')\n",
    "X_train_norm_aug, y_train_norm_aug = smote.fit_resample(X_train_norm, y_train)\n",
    "X_train_log_aug, y_train_log_aug = smote.fit_resample(X_train_log, y_train)\n",
    "X_train_aug, y_train_aug = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "value_counts = y_train_norm_aug.value_counts()\n",
    "print(\"Number of samples for each class: \\n\" + str(value_counts))\n",
    "\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffle_idx = np.random.permutation(len(X_train_norm_aug)) \n",
    "X_train_norm_aug = pd.DataFrame(np.array(X_train_norm_aug, dtype=float)[shuffle_idx], columns=column_names) # Normalized\n",
    "y_train_norm_aug = pd.Series(np.array(y_train_norm_aug, dtype=int)[shuffle_idx])\n",
    "\n",
    "X_train_log_aug = pd.DataFrame(np.array(X_train_log_aug, dtype=float)[shuffle_idx], columns=column_names) # log transformed\n",
    "y_train_log_aug = pd.Series(np.array(y_train_log_aug, dtype=int)[shuffle_idx])\n",
    "\n",
    "X_train_aug = pd.DataFrame(np.array(X_train_aug, dtype=float)[shuffle_idx], columns=column_names) # simple\n",
    "y_train_aug = pd.Series(np.array(y_train_aug, dtype=int)[shuffle_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8e6cc0",
   "metadata": {},
   "source": [
    "Try to perform undersampling, despite we have just 5009 samples in the least numerous class. Probably it won't produce good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "760c52c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples for each class: \n",
      "0    17491\n",
      "1     5009\n",
      "Name: default payment next month, dtype: int64\n",
      "Percentage of class 1 on the total: 22%\n",
      "Number of samples for each class: \n",
      "0    5009\n",
      "1    5009\n",
      "Name: default payment next month, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply RandomUnderSampler to undersample the dataset\n",
    "\"\"\" We used RandomUnderSampler and not other methods like Nearmiss as we have categorical features encoded as binary numerical features, \n",
    "and as it creates new samples based on distances of the given data, it can generate unwanted data for these categorical features.\n",
    "RandomUnderSampler instead is safe from these problems as it just resamples data from the given data, in our case without replacement, \n",
    "trying to keep the data as similar to the original as possible. \"\"\"\n",
    "value_counts = y_train.value_counts()\n",
    "tot_values = len(y_train)\n",
    "\n",
    "print(\"Number of samples for each class: \\n\" + str(value_counts))\n",
    "print(\"Percentage of class 1 on the total: \" + str(round(value_counts[1]/tot_values*100)) + \"%\")\n",
    "\n",
    "rand_unders_sampl = RandomUnderSampler(sampling_strategy=\"auto\")\n",
    "X_train_norm_und, y_train_norm_und = rand_unders_sampl.fit_resample(X_train_norm, y_train)\n",
    "X_train_log_und, y_train_log_und = rand_unders_sampl.fit_resample(X_train_log, y_train)\n",
    "X_train_und, y_train_und = rand_unders_sampl.fit_resample(X_train, y_train)\n",
    "\n",
    "value_counts = y_train_norm_und.value_counts()\n",
    "print(\"Number of samples for each class: \\n\" + str(value_counts))\n",
    "\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffle_idx = np.random.permutation(len(X_train_norm_und)) \n",
    "X_train_norm_und = pd.DataFrame(np.array(X_train_norm_und, dtype=float)[shuffle_idx], columns=column_names) # Normalized\n",
    "y_train_norm_und = pd.Series(np.array(y_train_norm_und, dtype=int)[shuffle_idx])\n",
    "\n",
    "X_train_log_und = pd.DataFrame(np.array(X_train_log_und, dtype=float)[shuffle_idx], columns=column_names) # log transformed\n",
    "y_train_log_und = pd.Series(np.array(y_train_log_und, dtype=int)[shuffle_idx])\n",
    "\n",
    "X_train_und = pd.DataFrame(np.array(X_train_und, dtype=float)[shuffle_idx], columns=column_names) # simple\n",
    "y_train_und = pd.Series(np.array(y_train_und, dtype=int)[shuffle_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8929ebea",
   "metadata": {},
   "source": [
    "Try to mix over and under sampling to reach a balanced dataset with not too many artificial samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf22cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples for each class: \n",
      "0    17491\n",
      "1     5009\n",
      "Name: default payment next month, dtype: int64\n",
      "Percentage of class 1 on the total: 22%\n",
      "Number of samples for each class: \n",
      "0    17491\n",
      "1     5247\n",
      "Name: default payment next month, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTENC to try to balance the dataset by oversampling\n",
    "value_counts = y_train.value_counts()\n",
    "tot_values = len(y_train)\n",
    "\n",
    "print(\"Number of samples for each class: \\n\" + str(value_counts))\n",
    "print(\"Percentage of class 1 on the total: \" + str(round(value_counts[1]/tot_values*100)) + \"%\")\n",
    "\n",
    "smote2 = SMOTENC(cat_columns, k_neighbors=5, sampling_strategy=0.3) # We tried many different percentages: np.linspace(0.0, 1.0, 10)\n",
    "#smote2= RandomOverSampler(sampling_strategy=0.3)\n",
    "X_train_norm_bal, y_train_norm_bal = smote2.fit_resample(X_train_norm, y_train)\n",
    "X_train_log_bal, y_train_log_bal = smote2.fit_resample(X_train_log, y_train)\n",
    "X_train_bal, y_train_bal = smote2.fit_resample(X_train, y_train)\n",
    "\n",
    "value_counts = y_train_bal.value_counts()\n",
    "print(\"Number of samples for each class: \\n\" + str(value_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f090f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples for each class: \n",
      "0    17491\n",
      "1     5247\n",
      "Name: default payment next month, dtype: int64\n",
      "Percentage of class 1 on the total: 23%\n",
      "Number of samples for each class: \n",
      "0    5247\n",
      "1    5247\n",
      "Name: default payment next month, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply RandomUnderSampler to undersample and balance the dataset\n",
    "value_counts = y_train_bal.value_counts()\n",
    "tot_values = len(y_train_bal)\n",
    "\n",
    "print(\"Number of samples for each class: \\n\" + str(value_counts))\n",
    "print(\"Percentage of class 1 on the total: \" + str(round(value_counts[1]/tot_values*100)) + \"%\")\n",
    "\n",
    "rand_unders_sampl = RandomUnderSampler(sampling_strategy=\"auto\")\n",
    "X_train_norm_bal, y_train_norm_bal = rand_unders_sampl.fit_resample(X_train_norm_bal, y_train_norm_bal)\n",
    "X_train_log_bal, y_train_log_bal = rand_unders_sampl.fit_resample(X_train_log_bal, y_train_log_bal)\n",
    "X_train_bal, y_train_bal = rand_unders_sampl.fit_resample(X_train_bal, y_train_bal)\n",
    "\n",
    "value_counts = y_train_norm_bal.value_counts()\n",
    "print(\"Number of samples for each class: \\n\" + str(value_counts))\n",
    "\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffle_idx = np.random.permutation(len(X_train_norm_bal)) \n",
    "X_train_norm_bal = pd.DataFrame(np.array(X_train_norm_bal, dtype=float)[shuffle_idx], columns=column_names) # Normalized\n",
    "y_train_norm_bal = pd.Series(np.array(y_train_norm_bal, dtype=int)[shuffle_idx])\n",
    "\n",
    "X_train_log_bal = pd.DataFrame(np.array(X_train_log_bal, dtype=float)[shuffle_idx], columns=column_names) # log transformed\n",
    "y_train_log_bal = pd.Series(np.array(y_train_log_bal, dtype=int)[shuffle_idx])\n",
    "\n",
    "X_train_bal = pd.DataFrame(np.array(X_train_bal, dtype=float)[shuffle_idx], columns=column_names) # simple\n",
    "y_train_bal = pd.Series(np.array(y_train_bal, dtype=int)[shuffle_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba3226d",
   "metadata": {},
   "source": [
    "### Test the performances of a Logistic Regression model with normalized and natural log transformed data. Both of them in 3 versions: oversampled, undersampled, a mix of the 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f393ebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled data:\n",
      "Accuracy with normalized augmented data:  0.6905333333333333\n",
      "Accuracy with natural log transformed augmented data:  0.6918666666666666\n",
      "sklearn f1_score on normalized augmented data:  0.470212280301301\n",
      "sklearn f1_score on natural log transformed augmented data:  0.47584486278067584\n",
      "\n",
      "Undersampled data:\n",
      "Accuracy with normalized augmented data:  0.696\n",
      "Accuracy with natural log transformed augmented data:  0.7010666666666666\n",
      "sklearn f1_score on normalized augmented data:  0.4739270881402861\n",
      "sklearn f1_score on natural log transformed augmented data:  0.47592332865825154\n",
      "\n",
      "Balanced data:\n",
      "Accuracy with normalized augmented data:  0.6898666666666666\n",
      "Accuracy with natural log transformed augmented data:  0.6890666666666667\n",
      "sklearn f1_score on normalized augmented data:  0.4706417842512517\n",
      "sklearn f1_score on natural log transformed augmented data:  0.47592332865825154\n"
     ]
    }
   ],
   "source": [
    "# Test a simple logistic regression with the different datasets\n",
    "# Create the different Logistic Regression models\n",
    "log_reg_norm_aug = LogisticRegression(random_state=42, max_iter=10000)\n",
    "log_reg_norm_und = LogisticRegression(random_state=42, max_iter=10000)\n",
    "log_reg_norm_bal = LogisticRegression(random_state=42, max_iter=10000)\n",
    "log_reg_log_aug = LogisticRegression(random_state=42, max_iter=10000)\n",
    "log_reg_log_und = LogisticRegression(random_state=42, max_iter=10000)\n",
    "log_reg_log_bal = LogisticRegression(random_state=42, max_iter=10000)\n",
    "\n",
    "# Fit the different Logistic regressions\n",
    "log_reg_norm_aug.fit(X_train_norm_aug, y_train_norm_aug)\n",
    "log_reg_norm_und.fit(X_train_norm_und, y_train_norm_und)\n",
    "log_reg_norm_bal.fit(X_train_norm_bal, y_train_norm_bal)\n",
    "log_reg_log_aug.fit(X_train_log_aug, y_train_log_aug)\n",
    "log_reg_log_und.fit(X_train_log_und, y_train_log_und)\n",
    "log_reg_log_bal.fit(X_train_log_bal, y_train_log_bal)\n",
    "\n",
    "\"\"\"Results using RandomOverSampler instead of SMOTENC:\n",
    "Oversampled data:\n",
    "Accuracy with normalized augmented data:  0.6885333333333333\n",
    "Accuracy with natural log transformed augmented data:  0.6896\n",
    "sklearn f1_score on normalized augmented data:  0.4729241877256318\n",
    "sklearn f1_score on natural log transformed augmented data:  0.4733031674208145\n",
    "\n",
    "Undersampled data:\n",
    "Accuracy with normalized augmented data:  0.6958666666666666\n",
    "Accuracy with natural log transformed augmented data:  0.6893333333333334\n",
    "sklearn f1_score on normalized augmented data:  0.4757527005286141\n",
    "sklearn f1_score on natural log transformed augmented data:  0.47069513857337575\n",
    "\n",
    "Balanced data:\n",
    "Accuracy with normalized augmented data:  0.6986666666666667\n",
    "Accuracy with natural log transformed augmented data:  0.6924\n",
    "sklearn f1_score on normalized augmented data:  0.47294776119402987\n",
    "sklearn f1_score on natural log transformed augmented data:  0.4734078977402419\n",
    "\"\"\"\n",
    "\n",
    "# Test the Logistic regressions\n",
    "print(\"Oversampled data:\")\n",
    "print(\"Accuracy with normalized augmented data: \", log_reg_norm_aug.score(X_test_norm, y_test))\n",
    "print(\"Accuracy with natural log transformed augmented data: \", log_reg_log_aug.score(X_test_log, y_test))\n",
    "y_pred_norm = log_reg_norm_aug.predict(X_test_norm)\n",
    "y_pred_log = log_reg_log_aug.predict(X_test_log)\n",
    "print(\"sklearn f1_score on normalized augmented data: \", f1_score(y_test, y_pred_norm))\n",
    "print(\"sklearn f1_score on natural log transformed augmented data: \", f1_score(y_test, y_pred_log))\n",
    "\n",
    "print(\"\\nUndersampled data:\")\n",
    "print(\"Accuracy with normalized augmented data: \", log_reg_norm_und.score(X_test_norm, y_test))\n",
    "print(\"Accuracy with natural log transformed augmented data: \", log_reg_log_und.score(X_test_log, y_test))\n",
    "y_pred_norm = log_reg_norm_und.predict(X_test_norm)\n",
    "y_pred_log = log_reg_log_und.predict(X_test_log)\n",
    "print(\"sklearn f1_score on normalized augmented data: \", f1_score(y_test, y_pred_norm))\n",
    "print(\"sklearn f1_score on natural log transformed augmented data: \", f1_score(y_test, y_pred_log))\n",
    "\n",
    "print(\"\\nBalanced data:\")\n",
    "print(\"Accuracy with normalized augmented data: \", log_reg_norm_bal.score(X_test_norm, y_test))\n",
    "print(\"Accuracy with natural log transformed augmented data: \", log_reg_log_bal.score(X_test_log, y_test))\n",
    "y_pred_norm_bal = log_reg_norm_bal.predict(X_test_norm)\n",
    "y_pred_log_bal = log_reg_log_bal.predict(X_test_log)\n",
    "print(\"sklearn f1_score on normalized augmented data: \", f1_score(y_test, y_pred_norm_bal))\n",
    "print(\"sklearn f1_score on natural log transformed augmented data: \", f1_score(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b4f93",
   "metadata": {},
   "source": [
    "### Test the performances of a Support Vector Classifier with rbf as kernel (non-linear) model with normalized and natural log transformed data. Both of them in 3 versions: oversampled, undersampled, a mix of the 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e947b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled data:\n",
      "Accuracy with normalized augmented data:  0.7826666666666666\n",
      "Accuracy with natural log transformed augmented data:  0.778\n",
      "sklearn f1_score on normalized augmented data:  0.5066585956416464\n",
      "sklearn f1_score on natural log transformed augmented data:  0.5037257824143069\n",
      "\n",
      "Undersampled data:\n",
      "Accuracy with normalized augmented data:  0.7826666666666666\n",
      "Accuracy with natural log transformed augmented data:  0.7858666666666667\n",
      "sklearn f1_score on normalized augmented data:  0.5027455765710799\n",
      "sklearn f1_score on natural log transformed augmented data:  0.5003111387678905\n",
      "\n",
      "Balanced data:\n",
      "Accuracy with normalized augmented data:  0.7878666666666667\n",
      "Accuracy with natural log transformed augmented data:  0.7830666666666667\n",
      "sklearn f1_score on normalized augmented data:  0.5010975227343994\n",
      "sklearn f1_score on natural log transformed augmented data:  0.5038121378469046\n"
     ]
    }
   ],
   "source": [
    "# Test an SVC model having non-linear kernel with the different datasets\n",
    "# Create the different SVC models\n",
    "svc_norm_aug = SVC(kernel='rbf', gamma='scale', max_iter=-1)\n",
    "svc_norm_und = SVC(kernel='rbf', gamma='scale', max_iter=-1)\n",
    "svc_norm_bal = SVC(kernel='rbf', gamma='scale', max_iter=-1)\n",
    "svc_log_aug = SVC(kernel='rbf', gamma='scale', max_iter=-1)\n",
    "svc_log_und = SVC(kernel='rbf', gamma='scale', max_iter=-1)\n",
    "svc_log_bal = SVC(kernel='rbf', gamma='scale', max_iter=-1)\n",
    "# svcs = [svc_norm_aug, svc_norm_und, svc_norm_bal, svc_log_aug, svc_log_und, svc_log_bal]\n",
    "\n",
    "# Fit the different SVCs\n",
    "svc_norm_aug.fit(X_train_norm_aug, y_train_norm_aug)\n",
    "svc_norm_und.fit(X_train_norm_und, y_train_norm_und)\n",
    "svc_norm_bal.fit(X_train_norm_bal, y_train_norm_bal)\n",
    "svc_log_aug.fit(X_train_log_aug, y_train_log_aug)\n",
    "svc_log_und.fit(X_train_log_und, y_train_log_und)\n",
    "svc_log_bal.fit(X_train_log_bal, y_train_log_bal)\n",
    "\n",
    "\"\"\"Results using RandomOverSampler instead of SMOTENC:\n",
    "Oversampled data:\n",
    "Accuracy with normalized augmented data:  0.7794666666666666\n",
    "Accuracy with natural log transformed augmented data:  0.7789333333333334\n",
    "sklearn f1_score on normalized augmented data:  0.5044937088076693\n",
    "sklearn f1_score on natural log transformed augmented data:  0.5044829647340108\n",
    "\n",
    "Undersampled data:\n",
    "Accuracy with normalized augmented data:  0.7869333333333334\n",
    "Accuracy with natural log transformed augmented data:  0.7794666666666666\n",
    "sklearn f1_score on normalized augmented data:  0.505569306930693\n",
    "sklearn f1_score on natural log transformed augmented data:  0.5024067388688328\n",
    "\n",
    "Balanced data:\n",
    "Accuracy with normalized augmented data:  0.7809333333333334\n",
    "Accuracy with natural log transformed augmented data:  0.7808\n",
    "sklearn f1_score on normalized augmented data:  0.5043740573152338\n",
    "sklearn f1_score on natural log transformed augmented data:  0.5051173991571343\n",
    "\"\"\"\n",
    "\n",
    "# Test the SVCs\n",
    "print(\"Oversampled data:\")\n",
    "print(\"Accuracy with normalized augmented data: \", svc_norm_aug.score(X_test_norm, y_test))\n",
    "print(\"Accuracy with natural log transformed augmented data: \", svc_log_aug.score(X_test_log, y_test))\n",
    "y_pred_norm = svc_norm_aug.predict(X_test_norm)\n",
    "y_pred_log = svc_log_aug.predict(X_test_log)\n",
    "print(\"sklearn f1_score on normalized augmented data: \", f1_score(y_test, y_pred_norm))\n",
    "print(\"sklearn f1_score on natural log transformed augmented data: \", f1_score(y_test, y_pred_log))\n",
    "\n",
    "print(\"\\nUndersampled data:\")\n",
    "print(\"Accuracy with normalized augmented data: \", svc_norm_und.score(X_test_norm, y_test))\n",
    "print(\"Accuracy with natural log transformed augmented data: \", svc_log_und.score(X_test_log, y_test))\n",
    "y_pred_norm = svc_norm_und.predict(X_test_norm)\n",
    "y_pred_log = svc_log_und.predict(X_test_log)\n",
    "print(\"sklearn f1_score on normalized augmented data: \", f1_score(y_test, y_pred_norm))\n",
    "print(\"sklearn f1_score on natural log transformed augmented data: \", f1_score(y_test, y_pred_log))\n",
    "\n",
    "print(\"\\nBalanced data:\")\n",
    "print(\"Accuracy with normalized augmented data: \", svc_norm_bal.score(X_test_norm, y_test))\n",
    "print(\"Accuracy with natural log transformed augmented data: \", svc_log_bal.score(X_test_log, y_test))\n",
    "y_pred_norm = svc_norm_bal.predict(X_test_norm)\n",
    "y_pred_log = svc_log_bal.predict(X_test_log)\n",
    "print(\"sklearn f1_score on normalized augmented data: \", f1_score(y_test, y_pred_norm))\n",
    "print(\"sklearn f1_score on natural log transformed augmented data: \", f1_score(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7da92f",
   "metadata": {},
   "source": [
    "We tested out many different percenteges for sampling_strategy in the making of the balanced dataset using both over and under sampling, but still using only oversampling performs better than the mix and than using undersampling.\n",
    "\n",
    "The results from the normalized and the natural log transformed dataset are very similar, but usually the log transformed one performs slightly better, so we'll keep it.\n",
    "\n",
    "We'll later on consider normalizing the already natural log transformed data for using certain models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "681d43ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the oversampled natural log transformed dataset\n",
    "current_path = os.getcwd()\n",
    "X_train_log_aug.to_pickle(current_path + \"/data/X_train_log_aug.pkl\")\n",
    "y_train_log_aug.to_pickle(current_path + \"/data/y_train_log_aug.pkl\")\n",
    "\n",
    "# X_train_norm_aug.to_pickle(current_path + \"/data/X_train_norm_aug.pkl\")\n",
    "# y_train_norm_aug.to_pickle(current_path + \"/data/y_train_norm_aug.pkl\")\n",
    "# X_train_log_aug.to_pickle(current_path + \"/data/X_train_log_aug.pkl\")\n",
    "# y_train_log_aug.to_pickle(current_path + \"/data/y_train_log_aug.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3e208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
